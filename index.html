<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Vedant Shah</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="logo">
							<span class="icon solid fa-laptop-code"></span>
						</div>
						<div class="content">
							<div class="inner">
								<h1>Vedant's Journey</h1>
								<p> Hello (Namaste), my name is Vedant Shah. Allow me to introduce myself to you :)</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#about">About Me</a></li>
								<li><a href="#research">Research </a></li>
								<li><a href="#projects">Projects</a></li>
								<li><a href="#contact">Contact</a></li>
								
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="about">
								<h2 class="major">About Me</h2>
								<span class="image main"><img src="images/pic01.jpg" style="filter:brightness(100%)" alt="" /></span>
								<p><h2>Hi, I'm Vedant Shah</h2> 
								    I'm a budding machine learning engineer with an ardent interest in deep learning. I'm an accelerated MS student at <a href=https://cs.vt.edu/> <b>Virginia Tech</b> </a> in the senior year of my Bachelors in Computer Science.
								</p>
								<p>
									Currently, I work with my research advisor <a href=https://isminoula.github.io/><b> Dr. Ismini Lourentzou</b></a> in the PLAN lab @ VT. My research interests lie in computer vision, in the areas of object detection; object recognition; pose estimation, 
									dataset quality, and dataset generation.
									I'll be completing my undergrad in May 2023 and continue onto my MS thesis thereafter. You can learn more about me through my <a href=#research><strong>publications and research work</strong></a>
									as well as my selected <a href=#projects><strong>personal projects</strong></a>.
								</p>
								<p>
									When I'm off research, I like going for a swim, working out, hiking, cycling, playing badminton, and making new friends. 
                                    The four most important values to me are accountability, commitment, compassion, and integrity.
								</p>
							</article>

						<!-- Research -->
							<article id="research">
								<h2 class="major">Research</h2>
								<h3>Publication:</h3>
								<span class="image main"><img src="images/pic03.jpg" style="filter:brightness(110%)" alt="" /></span>
								<p>
									<h3>
											[Data] Quality Lies In The Eyes Of The Beholder			
									</h3>
								Xavier Plemming, <strong>Vedant Shah</strong>, Ismini Lourentzou
								<br>
								Keywords: Datasets, Data Quality, Data Utility, Incomplete Data, User Survey, Data Analytics.
								<br>
								<span class="icon solid fa-bahai"> </span>
								<a href=https://dl.acm.org/doi/proceedings/10.1145/3529190> <b> ACM PETRA 2022</b> </a> Association for Computing Machinery Pervasive Technologies Related to Assistive Environments |
								<br>
								<div style="width:550px;height:200px;line-height:1.5em;position:relative;right:5px;overflow:auto;padding:5px;">
									Abstract: As large-scale machine learning models become more prevalent
									in assistive and pervasive technologies, the research community
									has started examining limitations and challenges that arise from
									training data, e.g., fairness, bias, and interpretability issues. To
									this end, data-centric approaches are increasingly prevailing over
									time, showing that high-quality data is a critical component in
									many applications. Several studies explore methods to define and
									improve data quality, however, no uniform definition exists. In
									this work, we present an empirical analysis of the multifaceted
									problem of evaluating data quality. Our work aims at identifying
									data quality challenges that are most commonly observed by data
									users and practitioners. Inspired by the need for generally applicable
									methods, we select a representative set of quality indicators, that
									covers a broad spectrum of issues, and investigate the utility of these
									indicators on a broad range of datasets through inter-annotator
									agreement analysis. Our work provides insights and presents open
									challenges in designing improved data life cycles.
								</div>
								<a href=https://dl.acm.org/doi/pdf/10.1145/3529190.3529222><span class="icon solid fa-file-pdf"> </span><strong>Paper</strong></a>
								&nbsp;
								<a href=https://doi.org/10.1145/3529190.3529222><span class="icon solid fa-search"> </span><strong>DOI</strong></a>
								</p>
								<p>
									<h3>
										Poster Presentation:				
									</h3>
									<span class="image main"><img src="images/pic04.jpg" style="filter:brightness(100%)"alt="" /></span>
									<strong> Vedant Shah</strong>, Julia WakeField, Anne Staples
									<br>
									<span class="icon solid fa-bahai"> </span>
									<a href=https://www.research.undergraduate.vt.edu/summer-research-vt/student-support/summer-research-sym.html><b> Virginia Tech Summer Research Symposium 2022 </b></a>
									<div style="width:550px;height:200px;line-height:1.5em;position:relative;right:5px;overflow:auto;padding:5px;">
										Abstract: A mechanical insect wing station has been constructed to understand the underlying fluid dynamic principles which guide the circulation of hemolymph in an insect wing. 
										The frequency at which the wing flaps depends on the voltage fed to the motor and on the inertial properties of the wing. Using machine learning (ML) to perform regression, 
										estimating the values of dependent variables based on independent variables, is a well-studied task both in academia and industry. While many generic regression models are available,
										we wanted to be able to calibrate the apparatus for new wings using only sparse data. A custom regression model was built using Tensorflow with the adaptive moment estimation algorithm 
										(Adam) as the optimization function and Huber as the loss function to learn the relationship between the voltage and frequency of specific insect wing models fabricated with stereolithography 
										resin printers. Datasets were prepared by manually tracking the flapping wing in video data in order to find the flapping frequency in Hertz and the voltage from the power source in Volts at 
										that frequency. The mean absolute percentage error is used to measure the accuracy of the prediction. The model predicts voltage based on input frequency with a 94% success rate. Automating 
										this manual task with an ML model improves the accuracy of values. This is essential to improve data accuracy in the insect wing hemodynamics experiments which has a broader application to 
										downstream tasks such as developing systems for drug delivery and other applications in human health.
									</div>
									<a href=https://drive.google.com/file/d/1Y6FPl0awdmT3tv6JXpTPmmz77JgBvgvp/view?usp=sharing><span class="icon solid fa-file-pdf"> </span><strong>High-Res Poster Pdf</strong></a>
									&nbsp;
									<a href=https://drive.google.com/file/d/1TeWqqgja_3F5vUgEf8ou9dlbHF9mF3Cn/view?usp=sharing><span class="icon solid fa-book"></span> <strong>Abstract [pg 203]</strong></a>
									&nbsp;
									<a href=https://github.com/sVedantWork/FINLAB-ML-BioMed-Research/blob/main/GrassHoper_Wing_Model.ipynb><span class="icon brands fa-github"></span> <strong> Code</strong></a>

									<span class="image main"><img src="images/pic05.jpg" style="filter:brightness(110%)" alt="" /></span>
									Adaliah Duyna, Cayla Catz, Jessica Prisbe, <strong>Vedant Shah</strong>, Shuyu Zhang, Anne Staples
									<br>
									<span class="icon solid fa-bahai"> </span>
									<a href=https://www.research.undergraduate.vt.edu/summer-research-vt/student-support/summer-research-sym.html><b> Virginia Tech Summer Research Symposium 2022 </b></a>
									<div style="width:550px;height:200px;line-height:1.5em;position:relative;right:5px;overflow:auto;padding:5px;">
										Abstract: Over 500 million people are affected by diabetes worldwide. For insulin-dependent patients, treatments include bulky and inconvenient battery-powered insulin pumps and painful syringe injections. 
										The InsulPatch is an alternative, smaller-scale insulin delivery pump currently under development. It provides an inexpensive way to administer insulin easily, painlessly, and without a power source,
										making insulin delivery more convenient for Type 1 diabetics. Inspired by the insect respiratory system, the InsulPatch features a multilayer microfluidic pump system. It uses the wearerâ€™s radial pulse to 
										pump insulin transdermally via a microneedle array. Photolithography and stereolithography (SLA) 3D printing microfabrication techniques were used to create device design molds. The molds were then used 
										to create the InsulPatch device layers by pouring liquid polydimethylsiloxane (PDMS) into the molds and baking to cure the PDMS. The devices contain three layers: a top layer consisting of actuation channels,
										a thin middle membrane, and a bottom layer containing the flow channels. Flow rate data was collected at different actuation pressures and frequencies for different flow and actuation channel geometric design 
										parameters. The devices were actuated using pressurized air signals representing different blood pressures and heart rates. The flow rate data was collected and analyzed using Graphpad Prism. This data was then
										used to create a sequential regression machine learning model with 80% prediction accuracy, enabling the solution of the inverse problem of producing device designs for specific patients. After successful clinical 
										trials, the InsulPatch will allow for accessible, painless, and convenient insulin delivery.
									</div>
									<a href=https://drive.google.com/file/d/1FvoaUCUgjZ7Mq3i3mdWjM5YGdXDSCOkw/view?usp=sharing><span class="icon solid fa-file-pdf"> </span><strong>High-Res Poster Pdf</strong></a>
									&nbsp;
									<a href=https://drive.google.com/file/d/1TeWqqgja_3F5vUgEf8ou9dlbHF9mF3Cn/view?usp=sharing><span class="icon solid fa-book"></span> <strong> Abstract [pg 123]</strong></a>
									&nbsp;
									<a href=https://github.com/sVedantWork/FINLAB-ML-BioMed-Research/blob/main/InsulPatch_ML_model.ipynb><span class="icon brands fa-github"></span> <strong> Code</strong></a>
			
								</p>	
							</article>

						<!-- Projects -->
							<article id="projects">
								<h2 class="major">Projects</h2>
								<h2>
									Gesture Control
								</h2>
									<br>
									<h4><li>
										Volume Control
									</li></h4>
								<span class="image main"><img src="images/vid1.gif" width="450" height="300" style="filter:brightness(110%)" alt="" /></span>
								<p>
								   Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and alter the sound of the host system. This version
								   is designed for desktop use.
								   <br>
								   <a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/VolumeHandControl.py><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								   &nbsp;
								   <a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4>
									<li>
										Virtual Mouse Control
								    </li>
							    </h4>
								<span class="image main"><img src="images/vid2.gif" width="450" height="300" style="filter:brightness(110%)" alt="" /></span>
								<p>
								Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and move the mouse pointer on the desktop.
								Further, this model also supports single and double clicks with certain finger movements. This version is designed for desktop use.
								<br>
								<a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/AI_Virtual_Mouse.py><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4>
									<li>
										Virtual Keyboard
								    </li>
							    </h4>
								<span class="image main"><img src="images/vid3.gif" width="450" height="300" style="filter:brightness(110%)" alt="" /></span>
								<p>
								Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and type the letters as selected on the desktop screen.
								This version is designed for desktop use and is able to write text on popular apps such as notepad++, Word, etc.
								<br>
								<a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/AI_KeyBoard.py><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4>
									<li>
										Virtual Painter
								    </li>
							    </h4>
								<span class="image main"><img src="images/vid4.gif" width="450" height="300" style="filter:brightness(110%)" alt="" /></span>
								<p>
								Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and paint on the desktop screen
								within the reference box. Currently it supports four different colors and the eraser option.
								This version is designed for desktop use.
								<br>
								<a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/AI_Painter.py><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href=https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>

								<h2>
									GAN & AutoEncoders
								</h2>
									<br>
									<h4><li>
										Variational AutoEncoder for Image Generation
									</li></h4>
								<figure>	
								<span class="image main"><img src="images/pic06.jpg" width="400" height="410" style="filter:brightness(100%)" alt="" /></span>
								<figcaption style="font-weight:200;">Img Title: Latent Space Distribution of the Variational AutoEncoder (VAE)</figcaption>
							    </figure>
								<br>
								<p>
								Brief : I've created this VAE from scratch using Tensorflow and Keras functional API for the task of dataset generation. This model
								is trained on the <a href="https://www.kaggle.com/datasets/zalando-research/fashionmnist"><em>fashion mnist dataset</em></a>. The figure above shows the latent space distribution of the model
								and the one below shows the results produced by my VAE.
								<br>
								<span class="image main"><img src="images/pic07.jpg" width="400" height="200" style="filter:brightness(100%)" alt="" /></span>
								<a href=https://github.com/sVedantWork/ImageGeneration/blob/main/VAE.ipynb><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href=https://github.com/sVedantWork/ImageGeneration/blob/main/README.md><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4><li>
									Variational AutoEncoder for Sound Generation
								</li></h4>
								<div>
									<strong>Original Sound</strong><br>
									<audio controls>
										<source src="sounds/og01.mp3" type="audio/mpeg">
									</audio>
									<br>
									<strong>Generated Sound</strong><br>
									<audio controls>
										<source src="sounds/gen01.mp3" type="audio/mpeg">
									</audio>
								</div><br>
								<p>
								Brief : I've created this VAE from scratch using Tensorflow and Keras functional API for the task of sound generation. This model
								is trained on the <a href="https://www.kaggle.com/datasets/joserzapata/free-spoken-digit-dataset-fsdd"><em>free spoken digits dataset</em></a>. The embedded audio above
								represents the orignal audio used for training the model and the audio generated by the model based on sampled mel spectograms.							    <br>
								<a href=https://github.com/sVedantWork/Sound-Generation><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href=https://github.com/sVedantWork/Sound-Generation/blob/main/README.md><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4><li>
									Generative Adversarial Network for Image Generation (On-Going)
								</li></h4>
								<figure>
							    <span class="image main"><img src="images/pic08.jpg" width="450" height="250" style="filter:brightness(100%)" alt="" /></span>
								<figcaption style="font-weight:200;">Img Title: Results produced by the GAN</figcaption>
							    </figure><br>
								<p>
								Brief : Currently, I'm working on applying my knowledge of GANs by building a simple GAN, from scratch, for the purpose of dataset generation. I'm using
								Tensorflow and Keras Sequential API to build this GAN. The image above indicates the results I've been able to obtain so far. This GAN is being trained on
								the <a href="https://www.kaggle.com/datasets/zalando-research/fashionmnist"><em>fashion mnist dataset</em></a>. I was able to handle the mode collapse issue with
								better training and currently I'm working on improvising the quality of the generated images.
								</main>
								<br>
								<a href=https://github.com/sVedantWork/ImageGeneration/blob/main/Generative_Adversarial_Networks.ipynb><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href=https://github.com/sVedantWork/ImageGeneration/blob/main/README.md><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
							</article>

						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								<div style="display:inline-block; vertical-align:top">
									<img src="images/pic02.jpg" width="260" height="520" alt="" />
								</div>
								&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
								&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
								<div style="display:inline-block;vertical-align:bottom;text-align:justify;font-size:x-large">
									<ul class="icons">
										<br><br>
										<li><a href="https://www.linkedin.com/in/vedant-s-64b114191/" class="icon brands fa-linkedin "><span class="label">LinkedIn</span></a></li>
										<strong>LinkedIn</strong>
										<br>
										<br>
										<br>
										<li><a href="https://github.com/sVedantWork?tab=repositories" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
										<strong>GitHub</strong>
										<br>
										<br>		
										<br>							
										<li><a href="mailto:vedant02@vt.edu" class="icon solid fa-envelope "><span class="label">Email</span></a></li>
										<strong>Email</strong>
										<br>
										<br>
										<br>
										<li><a href="https://drive.google.com/file/d/1o0VvfSLv7FwEw5EcsSsAFCKs-Sd2j6kv/view?usp=sharing" class="icon solid fa-file"><span class="label">Resume</span></a></li>
										<strong>Resume</strong>
									</ul>
								</div>	
							</article>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; 2022 by Vedant Shah designed with: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
